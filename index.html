<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Lihe Ding (‰∏ÅÁ´ãÈπ§)</title>
  
  <meta name="author" content="Lihe Ding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Lihe Ding</name>
              <p>
                I am a third-year Ph.D. candidate from <a href="https://mmlab.ie.cuhk.edu.hk/">MMLab</a>, The Chinese University of Hong Kong, and my advisor is Prof. <a href="https://tianfan.info/">Tianfan Xue</a>.
              </p>
              <p> 
                I received both my Bachelor's and Master's degrees from <a href="https://english.bit.edu.cn/">Beijing Institute of Technology</a>, supervised by Prof. <a href="https://scholar.google.com/citations?user=sQ_nP0ZaMn0C&hl=en">Jianan Li</a>. I spent a wonderful time in <a href="https://qcraft.ai/en">Qcraft</a> and SenseTime as a research intern, mentored by <a href="https://www.linkedin.com/in/boyin-zhang/">Boyin Zhang</a> and Dr. Zhanpeng Huang, respectively.
                I was a research assistant at Tsinghua University with Prof. <a href="https://ericyi.github.io/">Li Yi</a>.
              </p>
              <p style="text-align:center">
                <a href="data/lihe_CV.pdf">CV</a> &nbsp/&nbsp
<!--                 <a href="data/dinglihe-wechat.txt">Wechat</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=6nJrd8oAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
		<a href="https://twitter.com/DingLihe">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/pigtigger/">Github</a> 
		<br>
		<a href="mailto:dean.dinglihe@outlook.com">Email</a>: dean.dinglihe at outlook dot com
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/lihe_hawaii.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/lihe_hawaii.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I have been working on <span class="highlight">Neural Rendering</span> and <span class="highlight">Diffusion Model</span> for 3D generation. My previous work also included <span class="highlight">3D perception on Point Clouds</span>. (*: Equal Contribution ‚Ä†: Corresponding Author)
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr onmouseout="objnerf_stop()" onmouseover="objnerf_start()">
            <td style="padding:10px;width:40%;vertical-align:middle">
              <div class="one" style="width:100%">
                <div class="two" id='objnerf_image' style="width:100%"><video  width=100% muted autoplay loop>
                <source src="images/fullpart.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/fullpart.png' style="width:100%">
              </div>
              <script type="text/javascript">
                function adapt_start() {
                  document.getElementById('adapt_image').style.opacity = "1";
                }

                function adapt_stop() {
                  document.getElementById('adapt_image').style.opacity = "0";
                }
                adapt_stop()
              </script>
            </td>
                  <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://fullpart3d.github.io/">
                  <papertitle>FullPart: Generating each 3D Part at Full Resolution</papertitle>
                </a>
                <br>
                <strong>Lihe Ding*</strong>,
                Shaocong Dong*,
                Yaokun Li, Chenjian Gao, Xiao Chen, Rui Han, Yihao Kuang, Hong Zhang, Bo Huang, Zhanpeng Huang, Zibin Wang,
                Dan Xu‚Ä†,
                Tianfan Xue‚Ä†
                <!-- <a href="https://tianfan.info/">Tianfan Xue</a>‚Ä† -->
                <br>
          <em>ICLR</em>, 2026
                <br>
                <a href="https://fullpart3d.github.io/">project page</a>
          /
                <a href="https://arxiv.org/abs/2510.26140">paper</a>
          /
                <a href="https://github.com/hkdsc/fullpart">code</a>
                <p></p>
                <p>Fullpart generates each 3d part at full resolution. We also present PartVerse-XL, the largest human annotated 3d part dataset.</p>
              </td>
            </tr>

            <tr onmouseout="objnerf_stop()" onmouseover="objnerf_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <div class="one" style="width:100%">
                  <div class="two" id='objnerf_image' style="width:100%"><video  width=100% muted autoplay loop>
                  <source src="images/copart_opt.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <!-- <img src='images/objnerf.png' style="width:100%"> -->
                </div>
                <script type="text/javascript">
                  function objnerf_start() {
                    document.getElementById('objnerf_image').style.opacity = "1";
                  }
  
                  function objnerf_stop() {
                    document.getElementById('objnerf_image').style.opacity = "0";
                  }
                  objnerf_stop()
                </script>
              </td>
                    <td style="padding:10px;width:60%;vertical-align:middle">
                  <a href="https://hkdsc.github.io/project/copart/">
                    <papertitle>From One to More: Contextual Part Latents for 3D Generation</papertitle>
                  </a>
                  <br>
                  Shaocong Dong*,
                  <strong>Lihe Ding*</strong>,
                  Xiao Chen, Yaokun Li, Yuxin Wang, Yucheng Wang, Qi Wang, Jaehyeok Kim, Chenjian Gao, Zhanpeng Huang, Zibin Wang,
                  Tianfan Xue‚Ä†,
                  Dan Xu‚Ä†
                  <!-- <a href="https://tianfan.info/">Tianfan Xue</a>‚Ä† -->
                  <br>
            <em>ICCV</em>, 2025
                  <br>
                  <a href="https://hkdsc.github.io/project/copart/">project page</a>
            /
                  <a href="https://arxiv.org/abs/2507.08772">paper</a>
            /
                  <a href="https://huggingface.co/datasets/dscdyc/partverse">dataset</a>
                  <p></p>
                  <p>Copart generates 3d parts from contextual part latents and supports various application, such as articulation modeling.</p>
                </td>
              </tr>  

            <tr onmouseout="objnerf_stop()" onmouseover="objnerf_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <div class="one" style="width:100%">
                  <div class="two" id='objnerf_image' style="width:100%"><video  width=100% muted autoplay loop>
                  <source src="images/lora.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <!-- <img src='images/objnerf.png' style="width:100%"> -->
                </div>
                <script type="text/javascript">
                  function objnerf_start() {
                    document.getElementById('objnerf_image').style.opacity = "1";
                  }
  
                  function objnerf_stop() {
                    document.getElementById('objnerf_image').style.opacity = "0";
                  }
                  objnerf_stop()
                </script>
              </td>
                    <td style="padding:10px;width:60%;vertical-align:middle">
                  <a href="https://cjeen.github.io/LoRAEdit/">
                    <papertitle>LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning</papertitle>
                  </a>
                  <br>
                  Chenjian Gao, <strong>Lihe Ding</strong>, Xin Cai, Zhanpeng Huang, Zibin Wang, Tianfan Xue‚Ä†
                  <!-- <a href="https://tianfan.info/">Tianfan Xue</a>‚Ä† -->
                  <br>
            <em>ICLR</em>, 2026
                  <br>
                  <a href="https://cjeen.github.io/LoRAEdit/">project page</a>
            /
                  <a href="https://arxiv.org/abs/2506.10082">paper</a>
            /
                  <a href="https://github.com/cjeen/LoRAEdit">code</a>
                  <p></p>
                  <p>LoRA-Edit enables controllable video editing through mask-aware LoRA fine-tuning.</p>
                </td>
              </tr>
            
              <tr onmouseout="objnerf_stop()" onmouseover="objnerf_start()">
                <td style="padding:10px;width:40%;vertical-align:middle">
                  <div class="one" style="width:100%">
                    <div class="two" id='objnerf_image' style="width:100%"><video  width=100% muted autoplay loop>
                    <source src="images/tree_opt.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                    <!-- <img src='images/objnerf.png' style="width:100%"> -->
                  </div>
                  <script type="text/javascript">
                    function objnerf_start() {
                      document.getElementById('objnerf_image').style.opacity = "1";
                    }
    
                    function objnerf_stop() {
                      document.getElementById('objnerf_image').style.opacity = "0";
                    }
                    objnerf_stop()
                  </script>
                </td>
                      <td style="padding:10px;width:60%;vertical-align:middle">
                    <a href="https://dynamictree-dev.github.io/DynamicTree.github.io/">
                      <papertitle>DynamicTree: Interactive Real Tree Animation via Sparse Voxel Spectrum</papertitle>
                    </a>
                    <br>
                    Yaokun Li, <strong>Lihe Ding</strong>, Xiao Chen, Guang Tan,
                    Tianfan Xue‚Ä†
                    <!-- <a href="https://tianfan.info/">Tianfan Xue</a>‚Ä† -->
                    <br>
              <em>aXiv</em>, 2025
                    <br>
                    <a href="https://dynamictree-dev.github.io/DynamicTree.github.io/">project page</a>
              /
                    <a href="https://arxiv.org/abs/2510.22213">paper</a>
              
                    <p></p>
                    <p>we propose DynamicTree, the first framework that
                      can generate long-term, interactive animation of 3D Gaussian Splatting trees.</p>
                  </td>
                </tr>


            <tr onmouseout="objnerf_stop()" onmouseover="objnerf_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <div class="one" style="width:100%">
                  <div class="two" id='objnerf_image' style="width:100%"><video  width=100% muted autoplay loop>
                  <source src="images/gallery_opt.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <!-- <img src='images/objnerf.png' style="width:100%"> -->
                </div>
                <script type="text/javascript">
                  function objnerf_start() {
                    document.getElementById('objnerf_image').style.opacity = "1";
                  }
  
                  function objnerf_stop() {
                    document.getElementById('objnerf_image').style.opacity = "0";
                  }
                  objnerf_stop()
                </script>
              </td>
                    <td style="padding:10px;width:60%;vertical-align:middle">
                  <a href="https://cjeen.github.io/BraceletPaper/">
                    <papertitle>From Gallery to Wrist: Realistic 3D Bracelet Insertion in Videos</papertitle>
                  </a>
                  <br>
                  Chenjian Gao, <strong>Lihe Ding</strong>, Rui Han, Zhanpeng Huang, Zibin Wang,
                  Tianfan Xue‚Ä†
                  <!-- <a href="https://tianfan.info/">Tianfan Xue</a>‚Ä† -->
                  <br>
            <em>ICCV</em>, 2025
                  <br>
                  <a href="https://cjeen.github.io/BraceletPaper/">project page</a>
            /
                  <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Gao_From_Gallery_to_Wrist_Realistic_3D_Bracelet_Insertion_in_Videos_ICCV_2025_paper.pdf">paper</a>
            /
                  <a href="https://www.youtube.com/watch?v=mpwQFedhT_o">video</a>
                  <p></p>
                  <p>We propose a hybrid pipeline for inserting 3D objects into videos, combining 3D Gaussian Splatting rendering for temporal
                    consistency and a 2D diffusion-based enhancement for photorealistic lighting.</p>
                </td>
              </tr>
            
          
              <tr onmouseout="adapt_stop()" onmouseover="adapt_start()">
                <td style="padding:10px;width:40%;vertical-align:middle">
                  <div class="one" style="width:100%">
                    <div class="two" id='objnerf_image' style="width:100%"><video  width=100% muted autoplay loop>
                    <source src="images/er_2.png" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                    <img src='images/er_2.png' style="width:100%">
                  </div>
                  <script type="text/javascript">
                    function adapt_start() {
                      document.getElementById('adapt_image').style.opacity = "1";
                    }
    
                    function adapt_stop() {
                      document.getElementById('adapt_image').style.opacity = "0";
                    }
                    adapt_stop()
                  </script>
                </td>
                      <td style="padding:10px;width:60%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2508.13628">
                      <papertitle>DiffIER: Optimizing Diffusion Models with Iterative Error Reduction</papertitle>
                    </a>
                    <br>
                    Ao Chen,
                    <strong>Lihe Ding</strong>,
                    Tianfan Xue
                    <!-- <a href="https://tianfan.info/">Tianfan Xue</a>‚Ä† -->
                    <br>
              <em>aXiv</em>, 2025
                    <br>
                    <a href="https://arxiv.org/abs/2508.13628">paper</a>
                    <p></p>
                    <p>We present a general, training-free optimization method that achieves high-fidelity generation in diffusion
                      models.</p>
                  </td>
                </tr>

          <tr onmouseout="objnerf_stop()" onmouseover="objnerf_start()">
            <td style="padding:10px;width:40%;vertical-align:middle">
              <div class="one" style="width:100%">
                <div class="two" id='objnerf_image' style="width:100%"><video  width=100% muted autoplay loop>
                <source src="images/interactive3d.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/pokemon.gif' style="width:100%">
              </div>
              <script type="text/javascript">
                function objnerf_start() {
                  document.getElementById('objnerf_image').style.opacity = "1";
                }

                function objnerf_stop() {
                  document.getElementById('objnerf_image').style.opacity = "0";
                }
                objnerf_stop()
              </script>
            </td>
                  <td style="padding:10px;width:60%;vertical-align:middle">
                <a href="https://interactive-3d.github.io/">
                  <papertitle>Interactive3D: Create What You Want by Interactive 3D Generation</papertitle>
                </a>
                <br>
                Shaocong Dong*,
                <strong>Lihe Ding*</strong>,
                Zhanpeng Huang,
                Zibin Wang,
                Tianfan Xue‚Ä†,
                Dan Xu‚Ä†
                <!-- <a href="https://tianfan.info/">Tianfan Xue</a>‚Ä† -->
                <br>
          <em>CVPR</em>, 2024
                <br>
                <a href="https://interactive-3d.github.io/">project page</a>
          /
                <a href="https://arxiv.org/abs/2404.16510">paper</a>
          /
                <a href="https://www.youtube.com/watch?v=ZYSOonigv3s">video</a>
                <p></p>
                <p>Interactive3D grants users precise control over the generative process through extensive 3D interaction capabilities.</p>
              </td>
            </tr>

          <tr onmouseout="bidiff_stop()" onmouseover="bidiff_start()">
			      <td style="padding:10px;width:40%;vertical-align:middle">
			        <div class="one" style="width:100%">
			          <div class="two" id='bidiff_image' style="width:100%"></video  width=100% muted autoplay loop>
			          <img src="images/gundam.gif" style="width:100%">
			          
			          </video></div>
			          <img src='images/eagle2.gif' style="width:100%">
			        </div>
			        <script type="text/javascript">
			          function bidiff_start() {
			            document.getElementById('bidiff_image').style.opacity = "1";
			          }

			          function bidiff_stop() {
			            document.getElementById('bidiff_image').style.opacity = "0";
			          }
			          bidiff_stop()
			        </script>
			      </td>
			            <td style="padding:10px;width:60%;vertical-align:middle">
			          <a href="https://bidiff.github.io/">
			            <papertitle>Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors</papertitle>
			          </a>
			          <br>
			          <strong>Lihe Ding*</strong>,
			          Shaocong Dong*,
                Zhanpeng Huang,
			          Zibin Wang‚Ä†,
                Yiyuan Zhang,
                Kaixiong Gong,
			          Dan Xu,
                Tianfan Xue‚Ä†
			          <!-- <a href="https://tianfan.info/">Tianfan Xue</a>‚Ä† -->
			          <br>
			    <em>CVPR</em>, 2024
			          <br>
			          <a href="https://bidiff.github.io/">project page</a>
			    /
			          <a href="https://arxiv.org/abs/2312.04963">paper</a>
			    /
			          <a href="https://www.youtube.com/watch?v=3AHDbJlGKwY">video</a>
			          <p></p>
			          <p>We intergrate both 3D and 2D diffusion models with powerful priors into a unified framework with bidirectional guidance.</p>
			        </td>
			      </tr>
          
            <tr onmouseout="objnerf_stop()" onmouseover="objnerf_start()">
              <td style="padding:10px;width:40%;vertical-align:middle">
                <div class="one" style="width:100%">
                  <div class="two" id='objnerf_image' style="width:100%"><video  width=100% muted autoplay loop>
                  <source src="images/objnerf.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/objnerf.png' style="width:100%">
                </div>
                <script type="text/javascript">
                  function objnerf_start() {
                    document.getElementById('objnerf_image').style.opacity = "1";
                  }
  
                  function objnerf_stop() {
                    document.getElementById('objnerf_image').style.opacity = "0";
                  }
                  objnerf_stop()
                </script>
              </td>
                    <td style="padding:10px;width:60%;vertical-align:middle">
                  <a href="https://objnerf.github.io/">
                    <papertitle>Obj-NeRF: Extracting Object NeRFs from Multi-view Images</papertitle>
                  </a>
                  <br>
                  Zhiyi Li,
                  <strong>Lihe Ding</strong>,
                  Tianfan Xue
                  <!-- <a href="https://tianfan.info/">Tianfan Xue</a>‚Ä† -->
                  <br>
            <em>arXiv</em>, 2023
                  <br>
                  <a href="https://objnerf.github.io/">project page</a>
            /
                  <a href="https://arxiv.org/abs/2311.15291">paper</a>
            /
                  <a href="https://www.youtube.com/watch?v=VEwYrSPFatg">video</a>
                  <p></p>
                  <p>We propose Obj-NeRF, a novel comprehensive pipeline to extract object NeRFs from multi-view images, which can be used for NeRF editing tasks.</p>
                </td>
              </tr>
          
              <tr onmouseout="adapt_stop()" onmouseover="adapt_start()">
                <td style="padding:10px;width:40%;vertical-align:middle">
                  <div class="one" style="width:100%">
                    <div class="two" id='objnerf_image' style="width:100%"><video  width=100% muted autoplay loop>
                    <source src="images/adapt.jpg" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                    <img src='images/adapt.jpg' style="width:100%">
                  </div>
                  <script type="text/javascript">
                    function adapt_start() {
                      document.getElementById('adapt_image').style.opacity = "1";
                    }
    
                    function adapt_stop() {
                      document.getElementById('adapt_image').style.opacity = "0";
                    }
                    adapt_stop()
                  </script>
                </td>
                      <td style="padding:10px;width:60%;vertical-align:middle">
                    <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Sample-adaptive_Augmentation_for_Point_Cloud_Recognition_Against_Real-world_Corruptions_ICCV_2023_paper.pdf">
                      <papertitle>Sample-adaptive Augmentation for Point Cloud Recognition
                        Against Real-world Corruptions</papertitle>
                    </a>
                    <br>
                    Jie Wang,
                    <strong>Lihe Ding</strong>,
                    Tingfa Xu,
                    Shaocong Dong,
                    Xinli Xu,
                    Long Bai,
                    Jianan Li
                    <!-- <a href="https://tianfan.info/">Tianfan Xue</a>‚Ä† -->
                    <br>
              <em>ICCV</em>, 2023
                    <br>
                    <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Sample-adaptive_Augmentation_for_Point_Cloud_Recognition_Against_Real-world_Corruptions_ICCV_2023_paper.pdf">paper</a>
              /
                    <a href="https://github.com/Roywangj/AdaptPoint">code</a>
                    <p></p>
                    <p>we propose an alternative to make sample-adaptive transformations based on the structure of the sample to cope with potential corruption via an auto-augmentation framework.</p>
                  </td>
                </tr>

          <tr onmouseout="fhnet_stop()" onmouseover="fhnet_start()">
			      <td style="padding:10px;width:40%;vertical-align:middle">
			        <div class="one" style="width:100%">
			          <div class="two" id='fhnet_image' style="width:100%"><video  width=100% muted autoplay loop>
			          <source src="images/fhnet.mp4" type="video/mp4">
			          Your browser does not support the video tag.
			          </video></div>
			          <img src='images/fhnet.jpg' style="width:100%">
			        </div>
			        <script type="text/javascript">
			          function fhnet_start() {
			            document.getElementById('fhnet_image').style.opacity = "1";
			          }

			          function fhnet_stop() {
			            document.getElementById('fhnet_image').style.opacity = "0";
			          }
			          refnerf_stop()
			        </script>
			      </td>
			            <td style="padding:10px;width:60%;vertical-align:middle">
			          <a href="papers/FH-Net-ECCV2022.pdf">
			            <papertitle>FH-Net: A Fast Hierarchical Network for Scene Flow Estimation on Real-world Point Clouds</papertitle>
			          </a>
			          <br>
			          <strong>Lihe Ding*</strong>,
			          Shaocong Dong*,
			          Tingfa Xu‚Ä†,
			          Xinli Xu,
			          Jie Wang,
			          <a href="https://scholar.google.com/citations?user=sQ_nP0ZaMn0C&hl=en">Jianan Li</a>‚Ä†
			          <br>
			    <em>ECCV</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
			          <br>
			          <a href="https://github.com/pigtigger/FH-Net">project page</a>
			    /
			          <a href="papers/FH-Net-ECCV2022.pdf">paper</a>
			    /
			          <a href="https://drive.google.com/uc?export=download&id=1p5YfvI7B03psV671uBlJZTtq687vpgrC">video</a>
			          <p></p>
			          <p>We establish new lidar-scanned scene flow datasets and propose a fast and hierarchical network for real-world scene flow estimation.</p>
			        </td>
			      </tr>

          <tr onmouseout="cagroup_stop()" onmouseover="cagroup_start()">
            <td style="padding:10px;width:40%;vertical-align:middle">
              <div class="one" style="width:100%">
                <div class="two" id='cagroup_image' style="width:100%">
                  <img src='images/cagroup_after.png' style="width:100%"></div>
                <img src='images/cagroup.png' style="width:100%">
              </div>
              <script type="text/javascript">
                function cagroup_start() {
                  document.getElementById('cagroup_image').style.opacity = "1";
                }

                function cagroup_stop() {
                  document.getElementById('cagroup_image').style.opacity = "0";
                }
                cagroup_stop()
              </script>
            </td>
            <td style="padding:10px;width:60%;vertical-align:middle">
              <a href="papers/CAGroup3D-NeurIPS2022.pdf">
                <papertitle>CAGroup3D: Class-Aware Grouping for 3D Object Detection on Point Clouds</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=R3Av3IkAAAAJ&hl=en">Haiyang Wang</a>*,
	      <strong>Lihe Ding*</strong>,
              Shaocong Dong,
              <a href="https://shishaoshuai.com/">Shaoshuai Shi</a>‚Ä†, 
	      <a href="https://dblp.org/pid/152/6095.html">Aoxue Li</a>,
              <a href="https://scholar.google.com/citations?user=sQ_nP0ZaMn0C&hl=en">Jianan Li</a>,
              <a href="https://scholar.google.com/citations?user=XboZC1AAAAAJ&hl=en">Zhenguo Li</a>, 
              <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=VZHxoh8AAAAJ">Liwei Wang</a>‚Ä†
              <br>
              <em>NeurIPS</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2210.04264">paper</a>
              /
	      <a href="https://github.com/Haiyang-W/CAGroup3D">code</a>
              <p></p>
              <p>
              We propose a novel class-aware 3D proposal generation strategy and an efficient fully sparse convolutional 3D refinement module for vote-based Indoor 3D Detection.
              </p>
            </td>
          </tr>
					
          <tr onmouseout="mssvt_stop()" onmouseover="mssvt_start()">
            <td style="padding:10px;width:40%;vertical-align:middle">
              <div class="one" style="width:100%">
                <div class="two" id='mssvt_image' style="width:100%">
                    <img src='images/mssvt_after.png' style="width:100%"></div>
                <img src='images/mssvt.png' style="width:100%">
              </div>
              <script type="text/javascript">
                function mssvt_start() {
                  document.getElementById('mssvt_image').style.opacity = "1";
                }

                function mssvt_stop() {
                  document.getElementById('mssvt_image').style.opacity = "0";
                }
                mssvt_stop()
              </script>
            </td>
            <td style="padding:10px;width:60%;vertical-align:middle">
							<a href="papers/MsSVT-NeurIPS2022.pdf">
                <papertitle>MsSVT: Mixed-scale Sparse Voxel Transformer for 3D Object Detection on Point Clouds</papertitle>
              </a>
              <br>
              Shaocong Dong*, 
              <strong>Lihe Ding*</strong>, 
              <a href="https://scholar.google.com/citations?user=R3Av3IkAAAAJ&hl=en">Haiyang Wang</a>,
              Tingfa Xu‚Ä†,
              Xinli Xu,
              Jie Wang,
	      Ziyang Bian,
	      <a href="https://scholar.google.com/citations?hl=zh-CN&user=XK7uZYcAAAAJ">Ying Wang</a>,
	      <a href="https://scholar.google.com/citations?user=sQ_nP0ZaMn0C&hl=en">Jianan Li</a>‚Ä†
              <br>
              <em>NeurIPS</em>, 2022  
              <br> 
							<a href="papers/MsSVT-NeurIPS2022.pdf">paper</a> / 
							<a href="https://github.com/dscdyc/MsSVT">code</a>				
              <p></p>
              <p>We propose the first powerful 3D window-based transformer backbone on sparse 3D voxels leveraging mixed-scale information.</p>
            </td>
          </tr>
	  
	  <tr>
            <td style="padding:10px;width:40%;vertical-align:middle">
              <img src='images/fusionrcnn.png' style="width:100%">
            </td>
            <td style="padding:10px;width:60%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2209.10733">
                <papertitle>FusionRCNN: LiDAR-Camera Fusion for Two-stage 3D Object Detection</papertitle>
              </a>
              <br>
              Xinli Xu,
	      Shaocong Dong,
              <strong>Lihe Ding</strong>,
	      Jie Wang,
              Tingfa Xu,
	      <a href="https://scholar.google.com/citations?user=sQ_nP0ZaMn0C&hl=en">Jianan Li</a>‚Ä†
              <br>
              <em>arXiv</em>, 2022  
              <br>
							<a href="https://arxiv.org/abs/2209.10733">paper</a> /
		    					<a href="https://github.com/xxlbigbrother/Fusion-RCNN">code</a>
              <p></p>
              <p>We propose a novel multi-modality two-stage approach to effectively and efficiently fuse point clouds and camera images in the Regions of Interest(RoI).</p>
            </td>
          </tr>
	  
	  <tr>
            <td style="padding:10px;width:40%;vertical-align:middle">
              <img src='images/papooling.png' style="width:100%">
            </td>
            <td style="padding:10px;width:60%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2111.14067">
                <papertitle>PAPooling: Graph-based Position Adaptive Aggregation of Local Geometry in Point Clouds</papertitle>
              </a>
              <br>
              Jie Wang,
	      <a href="https://scholar.google.com/citations?user=sQ_nP0ZaMn0C&hl=en">Jianan Li</a>‚Ä†,
              <strong>Lihe Ding</strong>,
	      <a href="https://scholar.google.com/citations?hl=zh-CN&user=XK7uZYcAAAAJ">Ying Wang</a>,
              Tingfa Xu‚Ä†
              <br>
              <em>arXiv</em>, 2021  
              <br>
							<a href="https://arxiv.org/abs/2111.14067">paper</a>				
              <p></p>
              <p>The dynamic graph constructing strategy can well capture the fine-grained geometry ignored by previous aggregation operations.</p>
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Education</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
	
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cuhk.png">
            </td>
            <td width="75%" valign="center">
              PhD in Multimedia Lab (MMLab) @ The Chinese University of Hong Kong
              <br>
              Sep. 2023 - Now
              <br>
              Advisor: Prof.<a href="https://tianfan.info/">Tianfan Xue</a>
            </td>
          </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bit_small.png">
            </td>
            <td width="75%" valign="center">
              MSc in Opt-Electronics information Science and Engineeering @ Beijing Institute of Technology
              <br>
              Sep. 2020 - Jun. 2023
              <br>
              Advisor: Prof.<a href="https://scholar.google.com/citations?user=sQ_nP0ZaMn0C&hl=en">Jianan Li</a>
            </td>
          </tr>
		
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/bit_small.png"></td>
            <td width="75%" valign="center">
              BSc in Opt-Electronics information Science and Engineeering @ Beijing Institute of Technology
              <br>
              Sep. 2016 - Jun. 2020
	      <br>
	      GPA: 90.2/100 
            </td>
          </tr>
       
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>
		<table width="100%" align="center" border="0" cellpadding="20"><tbody>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sensetime.png">
            </td>
            <td width="75%" valign="center">
              Metaverse Video R&D at SenseTime
              <br>
              Research Intern
              <br>
              Text-to-3D Generation using both 2D and 3D priors
              <br>
              May, 2023 - Sep, 2023
	      <br>
	      Mentor: Dr. Zhanpeng Huang
	      <br>
	      3DAR Group
            </td>
          </tr>
			
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tsinghua_small.png">
            </td>
            <td width="75%" valign="center">
              <a href="https://iiis.tsinghua.edu.cn/">Institute for Interdisciplinary Information Sciences (IIIS)</a> at Tsinghua University
              <br>
              Research Assistant
              <br>
              3D Generation with diffusion model and implicit fuction
              <br>
              May, 2022 - Sep, 2022
	      <br>
	      Advisor: Prof. <a href="https://ericyi.github.io/">Li Yi</a>
	      <br>
	      3D Visual Computing and Machine Intelligence (3DVICI) Lab
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/qcraft.png"></td>
            <td width="75%" valign="center">
              <a href="https://qcraft.ai/en">Qcraft</a>
              <br>
              Research Intern
              <br>
              Beijing, China
              <br>
              May, 2021 - June, 2022
	      <br>
	      Mentor: <a href="https://www.linkedin.com/in/boyin-zhang/">Boyin Zhang</a>
              <br>
	      Perception & Machine Learning Group
            </td>
          </tr>
							
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 Template from <a href="https://github.com/jonbarron/jonbarron_website">JonBarron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
